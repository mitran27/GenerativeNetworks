{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitran27/GenerativeNetworks/blob/main/DiffusionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_RfiQiBvplzI"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module,Sequential,Conv2d,GroupNorm,SiLU,Identity,Linear,LayerNorm,MultiheadAttention,ConvTranspose2d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convBlock(in_dim,out_dim):\n",
        "  return Sequential([Conv2d(in_dim,out_dim,3,padding=1),GroupNorm(8,out_dim),SiLU()])\n",
        "def timeEmeddingMLP(dim):\n",
        "  return Sequential([Linear(dim,dim*4),SiLU(),Linear(dim*4,dim)])\n",
        "def FCN(dim):\n",
        "  return Sequential([Linear(dim,dim*4),SiLU(),Linear(dim*4,dim)])\n",
        "\n",
        "\n",
        "\n",
        "class ResnetLayer(Module):\n",
        "  def __init__(self, in_dim, out_dim, time_dim=None):\n",
        "\n",
        "    self.block1 = convBlock(in_dim, out_dim)\n",
        "    self.block2 = convBlock(in_dim, out_dim)\n",
        "\n",
        "    self.resBlock = convBlock(out_dim)  if(in_dim!=out_dim) else Identity\n",
        "    self.timeLinear = Sequential(SiLU(), Linear(time_dim, out_dim)) if time_dim else None\n",
        "\n",
        "  def forward(self,X,timeLatent):\n",
        "    y = self.block1(X)\n",
        "    if(self.timeEmbedding):\n",
        "      y = y + self.timeLinear(timeLatent)\n",
        "    y = self.block2(y)\n",
        "\n",
        "    y = y + self.resBlock(X)\n",
        "\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "gWnWMlZQtdNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetAttention(Module):\n",
        "  def __init__(self, dim, heads):\n",
        "\n",
        "    self.layerNorm1 = LayerNorm(dim)\n",
        "    self.layerNorm2 = LayerNorm(dim)\n",
        "\n",
        "    self.conv_in = Conv2d(dim, dim ,kernel_size=1, padding=0)\n",
        "    self.conv_out = Conv2d(dim, dim ,kernel_size=1, padding=0)\n",
        "\n",
        "    self.attn = MultiheadAttention(dim , heads)\n",
        "    self.fcn = FCN(dim)\n",
        "\n",
        "\n",
        "  def forward(self,X,timeStamp):\n",
        "\n",
        "    y = self.conv_in(X)\n",
        "\n",
        "    # convert image format to sequence format\n",
        "\n",
        "    b,c,h,w = y.shape\n",
        "\n",
        "    y = y.view((b, c, h * w))\n",
        "    y_r = y.transpose(-1,-2)\n",
        "\n",
        "    y = self.layerNorm1(y_r)\n",
        "    y = self.attn(y)\n",
        "\n",
        "    y_r = y + y_r\n",
        "\n",
        "    y = self.layerNorm2(y_r)\n",
        "    y = self.fcn(y)\n",
        "\n",
        "    y = y + y_r\n",
        "\n",
        "    y  = y.transpose(-1,-2)\n",
        "    y = y.view((b, c, h , w))\n",
        "\n",
        "    y = self.conv_out(y)\n",
        "\n",
        "    # transformer under unet have property to add the input at end\n",
        "\n",
        "    return y + X\n"
      ],
      "metadata": {
        "id": "lW6r9WBaqA9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SamplingBlock(indim, outdim, timedim, no_heads):\n",
        "  return Sequential([\n",
        "      ResnetLayer(indim, outdim, timedim),\n",
        "      ResnetLayer(outdim, outdim, timedim),\n",
        "      UnetAttention(outdim,no_heads),\n",
        "  ])\n",
        "def downSamplingBlock(indim, outdim, timedim, no_heads, down_sample=False):\n",
        "  model = SamplingBlock()\n",
        "  if(down_sample):\n",
        "     model.add(Conv2d(outdim, outdim, kernel_size=3, stride=2, padding=1))\n",
        "  return model\n",
        "def upSamplingBlock(indim, outdim, timedim, no_heads, upsample = False):\n",
        "  model = SamplingBlock()\n",
        "  if(upsample):\n",
        "     model.add(ConvTranspose2d(outdim, outdim, kernel_size=3, stride=2, padding=1))\n",
        "  return model\n",
        "\n",
        "def bottleNeckBlock(dim, heads):\n",
        "  return Sequential([\n",
        "      ResnetLayer(dim, dim, None),\n",
        "      UnetAttention(dim,heads),\n",
        "      ResnetLayer(dim, dim, None),\n",
        "  ])"
      ],
      "metadata": {
        "id": "jm5bEw6eVrZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetDownBlock(Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self,x):\n",
        "    #"
      ],
      "metadata": {
        "id": "OLfeU4fkrvKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}