{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitran27/GenerativeNetworks/blob/main/Dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJDE-4V5Erl6",
        "outputId": "0215c282-3d58-4fae-8bb3-2f2c53f42299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep 17 12:55:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xKpnYgeGgZd",
        "outputId": "9f98f73e-7e95-4ffd-fa9b-7febc330aea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.3\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFP9wWMwpyoj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid_w, grid_h = cols * w, rows * h\n",
        "    grid = Image.new(\"RGB\", size=(grid_w, grid_h))\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        x = i % cols * w\n",
        "        y = i // cols * h\n",
        "        grid.paste(img, box=(x, y))\n",
        "\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TJIGEffq_Zv",
        "outputId": "81be93c4-aacf-4a6a-ddb2-3e45dee21e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 70436, done.\u001b[K\n",
            "remote: Counting objects: 100% (9752/9752), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1294/1294), done.\u001b[K\n",
            "remote: Total 70436 (delta 9163), reused 8682 (delta 8363), pack-reused 60684 (from 1)\u001b[K\n",
            "Receiving objects: 100% (70436/70436), 49.16 MiB | 9.79 MiB/s, done.\n",
            "Resolving deltas: 100% (52193/52193), done.\n"
          ]
        }
      ],
      "source": [
        "# prompt: clone diffusers repo\n",
        "\n",
        "!git clone https://github.com/huggingface/diffusers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqyva7UCSB-j",
        "outputId": "20483fa3-5ab0-49b9-ba85-4938d84e62bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/images.zip\n",
            "   creating: class_images/\n",
            "  inflating: class_images/download (1).jpg  \n",
            "  inflating: class_images/download (2).jpg  \n",
            "  inflating: class_images/download (3).jpg  \n",
            "  inflating: class_images/download (4).jpg  \n",
            "  inflating: class_images/download.jpg  \n",
            "   creating: instance_images/\n",
            "  inflating: instance_images/download (1).jpg  \n",
            "  inflating: instance_images/download (2).jpg  \n",
            "  inflating: instance_images/download (3).jpg  \n",
            "  inflating: instance_images/download (4).jpg  \n",
            "  inflating: instance_images/download.jpg  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "COmVoBfb3oQ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O7teBULXTR_X"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"stabilityai/stable-diffusion-2\"\n",
        "PRECISION = \"fp16\"\n",
        "instance_prompt = 'A photo of a qswuejriy'\n",
        "class_prompt = 'A photo of a a woman'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rhjnsrmEYbCM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, PretrainedConfig\n",
        "from transformers import CLIPTextModel\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from diffusers.optimization import get_scheduler\n",
        "import bitsandbytes as bnb\n",
        "from accelerate.utils import ProjectConfiguration, set_seed\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pmcicdisYpKn"
      },
      "outputs": [],
      "source": [
        "class DreamBoothDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               Instance_dir,\n",
        "               instance_prompt,\n",
        "               tokenizer,\n",
        "               class_dir,\n",
        "               class_prompt,\n",
        "               img_size=512,\n",
        "               tokenizer_max_length=None) -> None:\n",
        "\n",
        "          self.tokenizer = tokenizer\n",
        "          self.instance_dir = Instance_dir\n",
        "          self.instance_prompt = instance_prompt\n",
        "          self.class_dir = class_dir\n",
        "          self.class_prompt = class_prompt\n",
        "          self.img_size = img_size\n",
        "          self.tokenizer_max_length = tokenizer_max_length\n",
        "\n",
        "          self.instance_images = os.listdir(self.instance_dir)\n",
        "          self.class_images = os.listdir(self.class_dir)\n",
        "\n",
        "          self._prepare_transforms();\n",
        "\n",
        "          self._prepare_instance_tokens()\n",
        "          self._prepare_class_tokens()\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.instance_images)\n",
        "\n",
        "  def _prepare_transforms(self):\n",
        "        self._image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(self.img_size),\n",
        "                transforms.CenterCrop(self.img_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "  def _prepare_instance_tokens(self):\n",
        "      max_length = self.tokenizer.model_max_length if self.tokenizer_max_length is None else self.tokenizer_max_length\n",
        "      instance_tokens = self.tokenizer(\n",
        "          self.instance_prompt,\n",
        "          truncation=True,\n",
        "          padding=\"max_length\",\n",
        "          max_length=max_length,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "      self.instance_data = {\n",
        "          \"instance_prompt_ids\": instance_tokens[\"input_ids\"],\n",
        "          \"instance_attention_mask\": instance_tokens[\"attention_mask\"],\n",
        "      }\n",
        "\n",
        "  def _prepare_class_tokens(self):\n",
        "        max_length = self.tokenizer.model_max_length if self.tokenizer_max_length is None else self.tokenizer_max_length\n",
        "        class_tokens = self.tokenizer(\n",
        "          self.class_prompt,\n",
        "          truncation=True,\n",
        "          padding=\"max_length\",\n",
        "          max_length=max_length,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "        self.class_data = {\n",
        "          \"class_prompt_ids\": class_tokens[\"input_ids\"],\n",
        "          \"aclss_ttention_mask\": class_tokens[\"attention_mask\"],\n",
        "      }\n",
        "\n",
        "  def pre_compute_embedding(self, text_encoder):\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      self.instance_data[\"instance_embedding\"] = text_encoder(\n",
        "          input_ids=self.instance_data[\"instance_prompt_ids\"].to(text_encoder.device),\n",
        "          attention_mask=self.instance_data[\"instance_attention_mask\"].to(text_encoder.device),\n",
        "          return_dict = False\n",
        "      )[0].detach()\n",
        "\n",
        "      self.class_data[\"class_embedding\"] = text_encoder(\n",
        "          input_ids=self.class_data[\"class_prompt_ids\"].to(text_encoder.device),\n",
        "          attention_mask=self.class_data[\"aclss_ttention_mask\"].to(text_encoder.device),\n",
        "          return_dict = False\n",
        "      )[0]\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self, index) :\n",
        "        index = index % len(self.instance_images)\n",
        "        instance_image = Image.open(os.path.join(self.instance_dir, self.instance_images[index]))\n",
        "        class_image = Image.open(os.path.join(self.class_dir, self.class_images[index]))\n",
        "\n",
        "        assert class_image.mode == \"RGB\" and instance_image.mode == \"RGB\"\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        data[\"instance_image\"] = self._image_transforms(instance_image)\n",
        "        data[\"class_image\"] = self._image_transforms(class_image)\n",
        "\n",
        "        data.update(self.instance_data)\n",
        "        data.update(self.class_data)\n",
        "\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J_g9o-G4HjxU"
      },
      "outputs": [],
      "source": [
        "def merge_class_instance(batch):\n",
        "  merged_batch = {}\n",
        "\n",
        "  images = [data[\"instance_image\"] for data in batch]\n",
        "  latents = [data[\"instance_embedding\"] for data in batch]\n",
        "\n",
        "  images += [data[\"class_image\"] for data in batch]\n",
        "  latents += [data[\"class_embedding\"] for data in batch]\n",
        "  merged_batch[\"images\"] = torch.stack(images).to(memory_format=torch.contiguous_format).float()\n",
        "  merged_batch[\"latents\"] = torch.cat(latents,dim=0)\n",
        "\n",
        "  return merged_batch\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCAfqfbzU5Ds",
        "outputId": "9bdc577a-da49-4071-fbf8-f46822df7412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPTextModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 1024)\n",
              "      (position_embedding): Embedding(77, 1024)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-22): 23 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if PRECISION==\"fp16\" else torch.float32\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\",variant=PRECISION)\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", variant=PRECISION, torch_dtype=dtype).to(device)\n",
        "text_encoder.requires_grad_(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DreamBoothDataset(\"/content/instance\",instance_prompt,tokenizer,\"/content/prior\",class_prompt)\n",
        "train_dataset.pre_compute_embedding(text_encoder)"
      ],
      "metadata": {
        "id": "YihMHvfCIaqL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEGXfOXCU2fK",
        "outputId": "b2128b13-f30d-4669-ff26-596c77a25e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: merge_class_instance(batch),\n",
        "    )\n",
        "for ste,batch in enumerate(train_dataloader):\n",
        "  print(batch.keys(),batch['images'].shape,batch['latents'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p6R1mq9YtQdJ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DLPipeline(ABC):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def generate(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def save(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def load(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z554EPPgtcWN"
      },
      "source": [
        "**Required Models for training Stable diffusion**\n",
        "\n",
        "1) Tokenizer : to Convert text to input embedding\n",
        "\n",
        "2) Text Encoder : prcess the text to latents representing the information of the context of the text\n",
        "\n",
        "3) Scheduler : Noise and denoise the images/latents for input to the diffusion model\n",
        "* Noise the input for training to predict the noise\n",
        "\n",
        "* Denoise during inference to remove the predicted noise from input noise to process with next step\n",
        "\n",
        "4)  VAE :encode the image to latent space\n",
        "\n",
        "5) Unet : take the noised latent and try to predict the noise\n",
        "\n",
        "\n",
        "\n",
        "**Training the Stable diffusion Model**\n",
        "\n",
        "1) use tokenizer to create propt tokens and pass it to CLIP encoder to create the context embedding\n",
        "\n",
        "2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oEiYHVgzZts-"
      },
      "outputs": [],
      "source": [
        "import itertools,math\n",
        "class DreamBoothPipeline(DLPipeline):\n",
        "  def __init__(self,training_configs,\n",
        "               train_clip = False,\n",
        "               generate_class_images=False,\n",
        "               n_class=0,\n",
        "               class_images_dir=None,\n",
        "               class_prompt=None,\n",
        "               accelerate=False):\n",
        "\n",
        "    print(\"*** Downloading and loading models ***\")\n",
        "\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.dtype = torch.float16 if PRECISION==\"fp16\" else torch.float32\n",
        "\n",
        "    #self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\",variant=PRECISION)\n",
        "    self.text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", variant=PRECISION, torch_dtype=self.dtype) if train_clip else None\n",
        "    self.unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\", variant=PRECISION)\n",
        "    self.vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\",  torch_dtype=self.dtype, variant=PRECISION)\n",
        "    self.scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "\n",
        "    self.text_encoder.requires_grad_(False) if train_clip else None\n",
        "    self.vae.requires_grad_(False)\n",
        "\n",
        "    if(training_configs[\"gradient_checkpointing\"]):\n",
        "      self.unet.enable_gradient_checkpointing()\n",
        "      if train_clip:\n",
        "        self.text_encoder.enable_gradient_checkpointing()\n",
        "\n",
        "\n",
        "    self.train_clip = train_clip\n",
        "    if train_clip:\n",
        "      params = itertools.chain(self.unet.parameters(), self.text_encoder.parameters())\n",
        "    else:\n",
        "      params = self.unet.parameters()\n",
        "    self.optimizer = bnb.optim.Adam8bit(params,\n",
        "                                          lr = training_configs[\"lr\"],\n",
        "                                          betas=training_configs[\"betas\"],\n",
        "                                          eps=training_configs[\"adam_epsilon\"],\n",
        "                                          weight_decay=training_configs[\"adam_weight_decay\"])\n",
        "\n",
        "\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=os.path.join(training_configs[\"dir\"],\"project\"), logging_dir=os.path.join(training_configs[\"dir\"],\"log\"))\n",
        "\n",
        "    self.accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=training_configs[\"gradient_accumulation_steps\"],\n",
        "        mixed_precision=training_configs[\"mixed_precision\"],\n",
        "        log_with=training_configs[\"report_to\"],\n",
        "        project_config=accelerator_project_config\n",
        "    )\n",
        "\n",
        "    self.lr_scheduler = get_scheduler(\n",
        "        training_configs[\"lr_scheduler\"],\n",
        "        optimizer=self.optimizer,\n",
        "        num_warmup_steps=training_configs[\"lr_warmup_steps\"] * self.accelerator.num_processes,\n",
        "        num_training_steps=training_configs[\"max_train_steps\"] * self.accelerator.num_processes,\n",
        "        num_cycles=training_configs[\"lr_num_cycles\"],\n",
        "        power=training_configs[\"lr_power\"]\n",
        "    )\n",
        "\n",
        "    if generate_class_images:\n",
        "      self.generate_class_images(class_images_dir, n_class, class_prompt)\n",
        "\n",
        "    self.accelerate = accelerate\n",
        "\n",
        "    if(self.accelerate):\n",
        "      self.unet, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
        "            self.unet, self.optimizer, self.lr_scheduler\n",
        "        )\n",
        "    self.vae = self.vae.to(self.device)\n",
        "    self.text_encoder = self.text_encoder.to(self.device) if train_clip else None\n",
        "\n",
        "    self.max_train_steps = training_configs[\"max_train_steps\"] # maximum iteration to be done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, train_dataloader):\n",
        "    dataloader = self.accelerator.prepare(train_dataloader)\n",
        "\n",
        "    if(self.accelerator.mixed_precision == \"fp16\"):\n",
        "      weight_dtype = torch.float16\n",
        "    else:\n",
        "      weight_dtype = torch.float32\n",
        "\n",
        "    self.vae.to(dtype = weight_dtype)\n",
        "    #self.text_encoder.to(dtype = weight_dtype)\n",
        "\n",
        "    epochs =  math.ceil(self.max_train_steps / len(dataloader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      self.unet.train()\n",
        "      for step, batch in enumerate(dataloader):\n",
        "\n",
        "\n",
        "        # Encode th image to latent vector using VAE\n",
        "        #torch.Size([4, 3, 512, 512])\n",
        "        image_latent = self.vae.encode(batch[\"images\"].to(dtype = weight_dtype)).latent_dist.sample()\n",
        "\n",
        "\n",
        "\n",
        "        image_latent = image_latent * self.vae.config.scaling_factor\n",
        "        #torch.Size([4, 4, 64, 64])\n",
        "\n",
        "        # create Noise in image\n",
        "        noise = torch.randn_like(image_latent)\n",
        "        #torch.Size([4, 4, 64, 64])\n",
        "\n",
        "\n",
        "        # create random time steps\n",
        "        timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (image_latent.shape[0],), device=self.device, dtype=torch.long)\n",
        "        #torch.Size([4])\n",
        "\n",
        "        # Add the noise to the image latent based on the time step\n",
        "        noisy_latent = self.scheduler.add_noise(image_latent, noise, timesteps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        encoder_hidden_states = batch[\"latents\"]\n",
        "        #torch.Size([4, 77, 1024])\n",
        "\n",
        "        pred_noise =self.unet(noisy_latent, timesteps, encoder_hidden_states)[0]  # predicts how much noise has been added from prev time step\n",
        "        # torch.Size([4, 4, 64, 64])\n",
        "\n",
        "        loss = F.mse_loss(pred_noise.float(),noise.float(),reduction=\"mean\")  # assuming class loss and instance loss have same weightage\n",
        "        print(1)\n",
        "\n",
        "        self.accelerator.backward(loss)\n",
        "        print(1)\n",
        "\n",
        "        self.accelerator.clip_grad_norm_(self.unet.parameters(),1.0)\n",
        "        print(1)\n",
        "        image_latent = image_latent.detach()\n",
        "        noisy_latent = noisy_latent.detach()\n",
        "        encoder_hidden_states = encoder_hidden_states.detach()\n",
        "        timesteps = timesteps.detach()\n",
        "        noise = noise.detach()\n",
        "        pred_noise = pred_noise.detach()\n",
        "        loss = loss.detach()\n",
        "\n",
        "\n",
        "        self.optimizer.step()\n",
        "        print(1)\n",
        "\n",
        "        self.lr_scheduler.step()\n",
        "        print(1)\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        print(epoch,step,loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def generate():\n",
        "    pass\n",
        "  def save():\n",
        "    pass\n",
        "  def load():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1HOVddBZ8klW"
      },
      "outputs": [],
      "source": [
        "configs = {\n",
        "    \"lr\" : 1e-6 ,\n",
        "    \"adam_beta1\" : 0.9,\n",
        "    \"adam_beta2\" : 0.999,\n",
        "    \"betas\":(0.9,0.999),\n",
        "    \"adam_weight_decay\" : 1e-2,\n",
        "    \"adam_epsilon\" : 1e-08,\n",
        "    \"lr_scheduler\" : 'constant',\n",
        "    \"lr_warmup_steps\": 0,\n",
        "    \"lr_num_cycles\": 1,\n",
        "    \"max_train_steps\":1200,\n",
        "    \"lr_power\" : 1.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"mixed_precision\": \"fp16\",\n",
        "    \"report_to\": \"wandb\",\n",
        "    \"gradient_checkpointing\":True,\n",
        "    \"dir\": \"/content/dreambooth\"\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDM1WtAZC3ku",
        "outputId": "9db69fa2-dfc1-4802-be91-47e80cabade6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Downloading and loading models ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "dreamBooth = DreamBoothPipeline(configs,False,False,accelerate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fnF8gGaczbo"
      },
      "outputs": [],
      "source": [
        "dreamBooth.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwwrHWPxc_RG"
      },
      "outputs": [],
      "source": [
        "DreamBoothPipeline.generate_class_images(\"/content/classimgs\",2,\"a photo of a man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIm3MZr8DKvD"
      },
      "outputs": [],
      "source": [
        "from diffusers import  AutoencoderKL\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class StableDiffusionModel:\n",
        "    def __init__(self, model_name, revision=\"fp16\", variant=\"fp16\"):\n",
        "        # Set device and dtype to fp16 if GPU is available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "        # Load model components\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\",variant=PRECISION)\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", variant=PRECISION, torch_dtype=self.dtype).to(self.device)\n",
        "        self.unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\", torch_dtype=self.dtype, variant=variant).to(self.device)\n",
        "        self.vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\",  torch_dtype=self.dtype, variant=variant).to(self.device)\n",
        "        self.scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
        "\n",
        "    def inference(self, prompt, num_steps=50, output_image_path=\"generated_image.png\"):\n",
        "        # Tokenize the input prompt\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "\n",
        "        # Encode the text input to get text embeddings\n",
        "        text_embeddings = self.text_encoder(inputs)[0]\n",
        "        print(text_embeddings.shape)\n",
        "        # Generate latent noise for the image\n",
        "        latents = torch.randn((1, self.unet.config.in_channels, 64, 64), device=self.device, dtype=self.dtype)\n",
        "\n",
        "        # Prepare for the denoising loop\n",
        "        self.scheduler.set_timesteps(num_steps)\n",
        "        latents = latents * self.scheduler.init_noise_sigma\n",
        "\n",
        "        # Denoising loop using the UNet model\n",
        "        for t in self.scheduler.timesteps:\n",
        "            latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # Decode the latents to pixel space using the VAE\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        # Convert the image to PIL format\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        image = Image.fromarray((image[0] * 255).astype(\"uint8\"))\n",
        "\n",
        "        # Save or display the generated image\n",
        "        image.save(output_image_path)\n",
        "        image.show()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create an instance of the StableDiffusionModel\n",
        "    sd_model = StableDiffusionModel(model_name=\"stabilityai/stable-diffusion-xl-base-1.0\", revision=\"fp16\", variant=\"fp16\")\n",
        "\n",
        "    # Run inference and generate an image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9umAGJOzTBz8"
      },
      "outputs": [],
      "source": [
        "    sd_model.inference(prompt=\"A scenic landscape with mountains during sunset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvl1HLpypTWe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNyg0NfypRR4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icf12keWatBK"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvSyFYHveqZd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "prompts = [\"A young beautiful women with medium hair with a short smile\",\n",
        "          \"face of beatiful young indian lady with long hair dressed in saree  \",\n",
        "          \"A young  indian women dressed in shirt with bob hair \",\n",
        "          \"A beautiful women dressed formally wearing sunglases\",\n",
        "           \"A  women with blue eyes and blonde hair dressed in jeans wearing bag\",\n",
        "         \"A  women with brown eyes and straight hair dressed in traditional sitting in temple\",\n",
        "\n",
        "]\n",
        "prompts = prompts * 10\n",
        "pipe.to(\"cuda\")\n",
        "index = 0\n",
        "for prompt in prompts:\n",
        "  images = pipe(prompt=prompt, num_inference_steps=35, num_images_per_prompt = 8)[0]\n",
        "  for i in range(len(images)):\n",
        "          images[i].save(os.path.join(\"/content/classimgs\", f\"generated_{index}.png\"))\n",
        "          index +=1\n",
        "del(pipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arS6pCtllwSw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rku0-AX6iJj3"
      },
      "outputs": [],
      "source": [
        "  @staticmethod\n",
        "  def generate_class_images(class_images_dir, n_class:int, class_prompt:str, device=\"cpu\"):\n",
        "    if not os.path.exists(class_images_dir):\n",
        "      os.mkdir(class_images_dir)\n",
        "\n",
        "    if n_class > len(os.listdir(class_images_dir)):\n",
        "      print(\"Generating class images\")\n",
        "\n",
        "      vae = AutoencoderKL.from_pretrained(\n",
        "          \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "          torch_dtype=torch.float16\n",
        "      )\n",
        "      pipe = DiffusionPipeline.from_pretrained(\n",
        "          \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "          vae=vae,\n",
        "          torch_dtype=torch.float16,\n",
        "          variant=\"fp16\",\n",
        "          use_safetensors=True,\n",
        "      )\n",
        "      pipe.to(\"cuda\");\n",
        "\n",
        "      base_SD_pipeline = DiffusionPipeline.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
        "\n",
        "      images = base_SD_pipeline(prompt=class_prompt, num_inference_steps=25, num_images_per_prompt=n_class-len(os.listdir(class_images_dir)))\n",
        "      for i in range(len(images)):\n",
        "        images[i].save(os.path.join(class_images_dir, f\"generated_{i}.png\"))\n",
        "      del(base_SD_pipeline)\n",
        "\n",
        "    else:\n",
        "      print(\"Class images already generated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvCkcoRDA5-z"
      },
      "outputs": [],
      "source": [
        "# prompt: zip this folder /content/classimgs\n",
        "\n",
        "!zip -r /content/classimgs.zip /content/classimgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8P3QdzhA9ff"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOIF2JMRyFTUOi/G8DLJ0IS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}