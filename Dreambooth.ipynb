{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitran27/GenerativeNetworks/blob/main/Dreambooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJDE-4V5Erl6",
        "outputId": "f43e83ce-df64-42b0-da84-6a2a888becbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  3 07:12:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xKpnYgeGgZd",
        "outputId": "d4f3f8b9-4994-462c-d848-d4641d929f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.3\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UFP9wWMwpyoj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid_w, grid_h = cols * w, rows * h\n",
        "    grid = Image.new(\"RGB\", size=(grid_w, grid_h))\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        x = i % cols * w\n",
        "        y = i // cols * h\n",
        "        grid.paste(img, box=(x, y))\n",
        "\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TJIGEffq_Zv",
        "outputId": "81be93c4-aacf-4a6a-ddb2-3e45dee21e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 70436, done.\u001b[K\n",
            "remote: Counting objects: 100% (9752/9752), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1294/1294), done.\u001b[K\n",
            "remote: Total 70436 (delta 9163), reused 8682 (delta 8363), pack-reused 60684 (from 1)\u001b[K\n",
            "Receiving objects: 100% (70436/70436), 49.16 MiB | 9.79 MiB/s, done.\n",
            "Resolving deltas: 100% (52193/52193), done.\n"
          ]
        }
      ],
      "source": [
        "# prompt: clone diffusers repo\n",
        "\n",
        "!git clone https://github.com/huggingface/diffusers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqyva7UCSB-j",
        "outputId": "02105731-9b25-482c-8d57-5bde37afdefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dreambooth_images.zip\n",
            "   creating: prior/\n",
            "  inflating: prior/generated_0.png   \n",
            "  inflating: prior/generated_1.png   \n",
            "  inflating: prior/generated_10.png  \n",
            "  inflating: prior/generated_101.png  \n",
            "  inflating: prior/generated_104.png  \n",
            "  inflating: prior/generated_105.png  \n",
            "  inflating: prior/generated_106.png  \n",
            "  inflating: prior/generated_107.png  \n",
            "  inflating: prior/generated_108.png  \n",
            "  inflating: prior/generated_11.png  \n",
            "  inflating: prior/generated_110.png  \n",
            "  inflating: prior/generated_111.png  \n",
            "  inflating: prior/generated_112.png  \n",
            "  inflating: prior/generated_115.png  \n",
            "  inflating: prior/generated_12.png  \n",
            "  inflating: prior/generated_129.png  \n",
            "  inflating: prior/generated_13.png  \n",
            "  inflating: prior/generated_130.png  \n",
            "  inflating: prior/generated_131.png  \n",
            "  inflating: prior/generated_133.png  \n",
            "  inflating: prior/generated_134.png  \n",
            "  inflating: prior/generated_135.png  \n",
            "  inflating: prior/generated_136.png  \n",
            "  inflating: prior/generated_138.png  \n",
            "  inflating: prior/generated_139.png  \n",
            "  inflating: prior/generated_14.png  \n",
            "  inflating: prior/generated_140.png  \n",
            "  inflating: prior/generated_141.png  \n",
            "  inflating: prior/generated_142.png  \n",
            "  inflating: prior/generated_143.png  \n",
            "  inflating: prior/generated_145.png  \n",
            "  inflating: prior/generated_146.png  \n",
            "  inflating: prior/generated_147.png  \n",
            "  inflating: prior/generated_148.png  \n",
            "  inflating: prior/generated_150.png  \n",
            "  inflating: prior/generated_154.png  \n",
            "  inflating: prior/generated_16.png  \n",
            "  inflating: prior/generated_163.png  \n",
            "  inflating: prior/generated_164.png  \n",
            "  inflating: prior/generated_165.png  \n",
            "  inflating: prior/generated_166.png  \n",
            "  inflating: prior/generated_169.png  \n",
            "  inflating: prior/generated_17.png  \n",
            "  inflating: prior/generated_170.png  \n",
            "  inflating: prior/generated_171.png  \n",
            "  inflating: prior/generated_172.png  \n",
            "  inflating: prior/generated_173.png  \n",
            "  inflating: prior/generated_174.png  \n",
            "  inflating: prior/generated_175.png  \n",
            "  inflating: prior/generated_179.png  \n",
            "  inflating: prior/generated_18.png  \n",
            "  inflating: prior/generated_181.png  \n",
            "  inflating: prior/generated_183.png  \n",
            "  inflating: prior/generated_190.png  \n",
            "  inflating: prior/generated_193.png  \n",
            "  inflating: prior/generated_194.png  \n",
            "  inflating: prior/generated_195.png  \n",
            "  inflating: prior/generated_198.png  \n",
            "  inflating: prior/generated_199.png  \n",
            "  inflating: prior/generated_2.png   \n",
            "  inflating: prior/generated_20.png  \n",
            "  inflating: prior/generated_21.png  \n",
            "  inflating: prior/generated_22.png  \n",
            "  inflating: prior/generated_23.png  \n",
            "  inflating: prior/generated_3.png   \n",
            "  inflating: prior/generated_33.png  \n",
            "  inflating: prior/generated_34.png  \n",
            "  inflating: prior/generated_39.png  \n",
            "  inflating: prior/generated_4.png   \n",
            "  inflating: prior/generated_40.png  \n",
            "  inflating: prior/generated_41.png  \n",
            "  inflating: prior/generated_44.png  \n",
            "  inflating: prior/generated_45.png  \n",
            "  inflating: prior/generated_46.png  \n",
            "  inflating: prior/generated_47.png  \n",
            "  inflating: prior/generated_5.png   \n",
            "  inflating: prior/generated_50.png  \n",
            "  inflating: prior/generated_58.png  \n",
            "  inflating: prior/generated_6.png   \n",
            "  inflating: prior/generated_64.png  \n",
            "  inflating: prior/generated_65.png  \n",
            "  inflating: prior/generated_66.png  \n",
            "  inflating: prior/generated_68.png  \n",
            "  inflating: prior/generated_69.png  \n",
            "  inflating: prior/generated_7.png   \n",
            "  inflating: prior/generated_70.png  \n",
            "  inflating: prior/generated_71.png  \n",
            "  inflating: prior/generated_73.png  \n",
            "  inflating: prior/generated_74.png  \n",
            "  inflating: prior/generated_75.png  \n",
            "  inflating: prior/generated_76.png  \n",
            "  inflating: prior/generated_77.png  \n",
            "  inflating: prior/generated_8.png   \n",
            "  inflating: prior/generated_80.png  \n",
            "  inflating: prior/generated_81.png  \n",
            "  inflating: prior/generated_82.png  \n",
            "  inflating: prior/generated_85.png  \n",
            "  inflating: prior/generated_86.png  \n",
            "  inflating: prior/generated_87.png  \n",
            "  inflating: prior/generated_92.png  \n",
            "  inflating: prior/generated_93.png  \n",
            "   creating: instance/\n",
            "  inflating: instance/inst_1.jpg     \n",
            "  inflating: instance/inst_10.jpg    \n",
            "  inflating: instance/inst_2.jpg     \n",
            "  inflating: instance/inst_3.jpg     \n",
            "  inflating: instance/inst_4.jpg     \n",
            "  inflating: instance/inst_5.jpg     \n",
            "  inflating: instance/inst_6.jpg     \n",
            "  inflating: instance/inst_7.jpg     \n",
            "  inflating: instance/inst_8.jpg     \n",
            "  inflating: instance/inst_9.jpg     \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/dreambooth_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "COmVoBfb3oQ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6MGIl7FhVk81"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O7teBULXTR_X"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"stabilityai/stable-diffusion-2\"\n",
        "PRECISION = \"fp16\"\n",
        "instance_prompt = 'A photo of a qswuejriy'\n",
        "class_prompt = 'A photo of a a woman'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rhjnsrmEYbCM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, PretrainedConfig\n",
        "from transformers import CLIPTextModel\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from diffusers.optimization import get_scheduler\n",
        "import bitsandbytes as bnb\n",
        "from accelerate.utils import ProjectConfiguration, set_seed\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pmcicdisYpKn"
      },
      "outputs": [],
      "source": [
        "class DreamBoothDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               Instance_dir,\n",
        "               instance_prompt,\n",
        "               tokenizer,\n",
        "               class_dir,\n",
        "               class_prompt,\n",
        "               img_size=512,\n",
        "               tokenizer_max_length=None) -> None:\n",
        "\n",
        "          self.tokenizer = tokenizer\n",
        "          self.instance_dir = Instance_dir\n",
        "          self.instance_prompt = instance_prompt\n",
        "          self.class_dir = class_dir\n",
        "          self.class_prompt = class_prompt\n",
        "          self.img_size = img_size\n",
        "          self.tokenizer_max_length = tokenizer_max_length\n",
        "\n",
        "          self.instance_images = os.listdir(self.instance_dir)\n",
        "          self.class_images = os.listdir(self.class_dir)\n",
        "\n",
        "          self._prepare_transforms();\n",
        "\n",
        "          self._prepare_instance_tokens()\n",
        "          self._prepare_class_tokens()\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.class_images)\n",
        "\n",
        "  def _prepare_transforms(self):\n",
        "        self._image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(self.img_size),\n",
        "                transforms.CenterCrop(self.img_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "  def _prepare_instance_tokens(self):\n",
        "      max_length = self.tokenizer.model_max_length if self.tokenizer_max_length is None else self.tokenizer_max_length\n",
        "      instance_tokens = self.tokenizer(\n",
        "          self.instance_prompt,\n",
        "          truncation=True,\n",
        "          padding=\"max_length\",\n",
        "          max_length=max_length,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "      self.instance_data = {\n",
        "          \"instance_prompt_ids\": instance_tokens[\"input_ids\"],\n",
        "          \"instance_attention_mask\": instance_tokens[\"attention_mask\"],\n",
        "      }\n",
        "\n",
        "  def _prepare_class_tokens(self):\n",
        "        max_length = self.tokenizer.model_max_length if self.tokenizer_max_length is None else self.tokenizer_max_length\n",
        "        class_tokens = self.tokenizer(\n",
        "          self.class_prompt,\n",
        "          truncation=True,\n",
        "          padding=\"max_length\",\n",
        "          max_length=max_length,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "        self.class_data = {\n",
        "          \"class_prompt_ids\": class_tokens[\"input_ids\"],\n",
        "          \"aclss_ttention_mask\": class_tokens[\"attention_mask\"],\n",
        "      }\n",
        "\n",
        "  def pre_compute_embedding(self, text_encoder):\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      self.instance_data[\"instance_embedding\"] = text_encoder(\n",
        "          input_ids=self.instance_data[\"instance_prompt_ids\"].to(text_encoder.device),\n",
        "          attention_mask=self.instance_data[\"instance_attention_mask\"].to(text_encoder.device),\n",
        "          return_dict = False\n",
        "      )[0].detach()\n",
        "\n",
        "      self.class_data[\"class_embedding\"] = text_encoder(\n",
        "          input_ids=self.class_data[\"class_prompt_ids\"].to(text_encoder.device),\n",
        "          attention_mask=self.class_data[\"aclss_ttention_mask\"].to(text_encoder.device),\n",
        "          return_dict = False\n",
        "      )[0]\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self, index) :\n",
        "        instance_image = Image.open(os.path.join(self.instance_dir, self.instance_images[index % len(self.instance_images)]))\n",
        "        class_image = Image.open(os.path.join(self.class_dir, self.class_images[index]))\n",
        "\n",
        "        assert class_image.mode == \"RGB\" and instance_image.mode == \"RGB\"\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        data[\"instance_image\"] = self._image_transforms(instance_image)\n",
        "        data[\"class_image\"] = self._image_transforms(class_image)\n",
        "\n",
        "        data.update(self.instance_data)\n",
        "        data.update(self.class_data)\n",
        "\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J_g9o-G4HjxU"
      },
      "outputs": [],
      "source": [
        "def merge_class_instance(batch):\n",
        "  merged_batch = {}\n",
        "\n",
        "  images = [data[\"instance_image\"] for data in batch]\n",
        "  latents = [data[\"instance_embedding\"] for data in batch]\n",
        "\n",
        "  images += [data[\"class_image\"] for data in batch]\n",
        "  latents += [data[\"class_embedding\"] for data in batch]\n",
        "  merged_batch[\"images\"] = torch.stack(images).to(memory_format=torch.contiguous_format).float()\n",
        "  merged_batch[\"latents\"] = torch.cat(latents,dim=0)\n",
        "\n",
        "  return merged_batch\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCAfqfbzU5Ds",
        "outputId": "0883ab3d-da36-4b76-c575-c244846207a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPTextModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 1024)\n",
              "      (position_embedding): Embedding(77, 1024)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-22): 23 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if PRECISION==\"fp16\" else torch.float32\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\",variant=PRECISION)\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", variant=PRECISION, torch_dtype=dtype).to(device)\n",
        "text_encoder.requires_grad_(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YihMHvfCIaqL"
      },
      "outputs": [],
      "source": [
        "train_dataset = DreamBoothDataset(\"/content/instance\",instance_prompt,tokenizer,\"/content/prior\",class_prompt)\n",
        "train_dataset.pre_compute_embedding(text_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEGXfOXCU2fK",
        "outputId": "132349b0-09ff-4857-86e4-3d38a93473b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([4, 3, 512, 512]) torch.Size([4, 77, 1024])\n",
            "dict_keys(['images', 'latents']) torch.Size([2, 3, 512, 512]) torch.Size([2, 77, 1024])\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: merge_class_instance(batch),\n",
        "    )\n",
        "for ste,batch in enumerate(train_dataloader):\n",
        "  print(batch.keys(),batch['images'].shape,batch['latents'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p6R1mq9YtQdJ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DLPipeline(ABC):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def generate(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def save(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def load(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z554EPPgtcWN"
      },
      "source": [
        "**Required Models for training Stable diffusion**\n",
        "\n",
        "1) Tokenizer : to Convert text to input embedding\n",
        "\n",
        "2) Text Encoder : prcess the text to latents representing the information of the context of the text\n",
        "\n",
        "3) Scheduler : Noise and denoise the images/latents for input to the diffusion model\n",
        "* Noise the input for training to predict the noise\n",
        "\n",
        "* Denoise during inference to remove the predicted noise from input noise to process with next step\n",
        "\n",
        "4)  VAE :encode the image to latent space\n",
        "\n",
        "5) Unet : take the noised latent and try to predict the noise\n",
        "\n",
        "\n",
        "\n",
        "**Training the Stable diffusion Model**\n",
        "\n",
        "1) use tokenizer to create propt tokens and pass it to CLIP encoder to create the context embedding\n",
        "\n",
        "2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1v2OFe4-2yQm"
      },
      "outputs": [],
      "source": [
        "def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):\n",
        "    \"\"\"\n",
        "    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and\n",
        "    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4\n",
        "    \"\"\"\n",
        "    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)), keepdim=True)\n",
        "    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n",
        "    # rescale the results from guidance (fixes overexposure)\n",
        "    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n",
        "    # mix with the original results from guidance by factor guidance_rescale to avoid \"plain looking\" images\n",
        "    noise_cfg = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n",
        "    return noise_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oEiYHVgzZts-"
      },
      "outputs": [],
      "source": [
        "from os.path import exists\n",
        "import itertools,math,random\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "class DreamBoothPipeline(DLPipeline):\n",
        "  def __init__(self,training_configs,\n",
        "               train_clip = False,\n",
        "               generate_class_images=False,\n",
        "               n_class=0,\n",
        "               class_images_dir=None,\n",
        "               class_prompt=None,\n",
        "               accelerate=False):\n",
        "\n",
        "    print(\"*** Downloading and loading models ***\")\n",
        "\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.dtype = torch.float16 if PRECISION==\"fp16\" else torch.float32\n",
        "\n",
        "    #self.tokenizer =AutoTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\",variant=PRECISION)\n",
        "    self.text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", variant=PRECISION) if train_clip else None\n",
        "    self.unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\", variant=PRECISION)\n",
        "    self.vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\", variant=PRECISION)\n",
        "    self.scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "\n",
        "    self.text_encoder.requires_grad_(False) if train_clip else None\n",
        "    self.vae.requires_grad_(False)\n",
        "\n",
        "    if(training_configs[\"gradient_checkpointing\"]):\n",
        "      self.unet.enable_gradient_checkpointing()\n",
        "      if train_clip:\n",
        "        self.text_encoder.enable_gradient_checkpointing()\n",
        "\n",
        "\n",
        "    self.train_clip = train_clip\n",
        "    if train_clip:\n",
        "      params = itertools.chain(self.unet.parameters(), self.text_encoder.parameters())\n",
        "    else:\n",
        "      params = self.unet.parameters()\n",
        "    self.optimizer = bnb.optim.Adam8bit(params,\n",
        "                                          lr = training_configs[\"lr\"],\n",
        "                                          betas=training_configs[\"betas\"],\n",
        "                                          eps=training_configs[\"adam_epsilon\"],\n",
        "                                          weight_decay=training_configs[\"adam_weight_decay\"])\n",
        "\n",
        "\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=os.path.join(training_configs[\"dir\"],\"project\"), logging_dir=os.path.join(training_configs[\"dir\"],\"log\"))\n",
        "\n",
        "    self.accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=training_configs[\"gradient_accumulation_steps\"],\n",
        "        mixed_precision=training_configs[\"mixed_precision\"],\n",
        "        log_with=training_configs[\"report_to\"],\n",
        "        project_config=accelerator_project_config\n",
        "    )\n",
        "\n",
        "    self.lr_scheduler = get_scheduler(\n",
        "        training_configs[\"lr_scheduler\"],\n",
        "        optimizer=self.optimizer,\n",
        "        num_warmup_steps=training_configs[\"lr_warmup_steps\"] * self.accelerator.num_processes,\n",
        "        num_training_steps=training_configs[\"max_train_steps\"] * self.accelerator.num_processes,\n",
        "        num_cycles=training_configs[\"lr_num_cycles\"],\n",
        "        power=training_configs[\"lr_power\"]\n",
        "    )\n",
        "\n",
        "    if generate_class_images:\n",
        "      self.generate_class_images(class_images_dir, n_class, class_prompt)\n",
        "\n",
        "    self.accelerate = accelerate\n",
        "\n",
        "    if(self.accelerate):\n",
        "      self.unet, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
        "            self.unet, self.optimizer, self.lr_scheduler\n",
        "        )\n",
        "    self.vae = self.vae.to(self.device)\n",
        "    self.text_encoder = self.text_encoder.to(self.device) if train_clip else None\n",
        "\n",
        "    self.max_train_steps = training_configs[\"max_train_steps\"] # maximum iteration to be done\n",
        "\n",
        "    self.vae_image_processor = VaeImageProcessor(vae_scale_factor=self.vae.config.scaling_factor)\n",
        "    self.guidance_scale = 3.0\n",
        "\n",
        "\n",
        "  def unwrap_model(self,model):\n",
        "        model = self.accelerator.unwrap_model(model)\n",
        "        return model\n",
        "\n",
        "  def train(self, train_dataloader):\n",
        "    dataloader = self.accelerator.prepare(train_dataloader)\n",
        "\n",
        "    if(self.accelerator.mixed_precision == \"fp16\"):\n",
        "      weight_dtype = torch.float16\n",
        "    else:\n",
        "      weight_dtype = torch.float32\n",
        "\n",
        "    self.vae.to(dtype = weight_dtype)\n",
        "    if self.train_clip:\n",
        "      self.text_encoder.to(dtype = weight_dtype)\n",
        "\n",
        "    epochs =  math.ceil(self.max_train_steps / len(dataloader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      self.unet.train()\n",
        "      epoch_loss = 0\n",
        "      with tqdm(total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False) as pbar:\n",
        "        for step, batch in enumerate(dataloader):\n",
        "\n",
        "\n",
        "          # Encode th image to latent vector using VAE\n",
        "          #torch.Size([4, 3, 512, 512])\n",
        "          image_latent = self.vae.encode(batch[\"images\"].to(dtype = weight_dtype)).latent_dist.sample()\n",
        "\n",
        "\n",
        "\n",
        "          image_latent = image_latent * self.vae.config.scaling_factor\n",
        "          #torch.Size([4, 4, 64, 64])\n",
        "\n",
        "          # create Noise in image\n",
        "          noise = torch.randn_like(image_latent)\n",
        "          #torch.Size([4, 4, 64, 64])\n",
        "\n",
        "\n",
        "          # create random time steps\n",
        "          timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (image_latent.shape[0],), device=self.device, dtype=torch.long)\n",
        "          #torch.Size([4])\n",
        "\n",
        "          # Add the noise to the image latent based on the time step\n",
        "          noisy_latent = self.scheduler.add_noise(image_latent, noise, timesteps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          encoder_hidden_states = batch[\"latents\"]\n",
        "          #torch.Size([4, 77, 1024])\n",
        "\n",
        "          pred_noise =self.unet(noisy_latent, timesteps, encoder_hidden_states)[0]  # predicts how much noise has been added from prev time step\n",
        "          # torch.Size([4, 4, 64, 64])\n",
        "\n",
        "          loss = F.mse_loss(pred_noise.float(),noise.float(),reduction=\"mean\")  # assuming class loss and instance loss have same weightage\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          self.accelerator.backward(loss)\n",
        "\n",
        "          self.accelerator.clip_grad_norm_(self.unet.parameters(),1.0)\n",
        "\n",
        "          self.optimizer.step()\n",
        "\n",
        "          self.lr_scheduler.step()\n",
        "          self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "          pbar.set_postfix({\"loss\": loss.item(), \"avg_loss\": epoch_loss / (step + 1)})\n",
        "          pbar.update(1)\n",
        "        print(f\"Epoch {epoch + 1} loss: {epoch_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def generate(self,prompt,images_per_prompt,noise=None,diffusion_steps=50,tokenizer=None,text_encoder=None):\n",
        "\n",
        "\n",
        "    unet=self.unwrap_model(self.unet)\n",
        "    generator = torch.cuda.manual_seed(random.randrange(2**32 - 1))\n",
        "\n",
        "    init_latent = torch.zeros((images_per_prompt, unet.in_channels, 512 // 8, 512 // 8), device=self.device)\n",
        "    t_start = torch.tensor(0)\n",
        "\n",
        "    self.scheduler.set_timesteps(diffusion_steps)\n",
        "    noise = torch.randn(init_latent.shape, generator=generator, device=self.device)\n",
        "    latents = noise * self.scheduler.init_noise_sigma\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      if tokenizer:\n",
        "        self.tokenizer = tokenizer\n",
        "      if text_encoder:\n",
        "        self.text_encoder = text_encoder\n",
        "\n",
        "      inputs = self.tokenizer([prompt]*images_per_prompt, return_tensors=\"pt\",padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True,).input_ids.to(self.device)\n",
        "      text_embeddings = self.text_encoder(inputs)[0]\n",
        "\n",
        "      uncond_inputs = self.tokenizer(\n",
        "          [\"\"] * images_per_prompt, return_tensors=\"pt\",padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True,\n",
        "      ).input_ids.to(self.device)\n",
        "      uncond_embeddings = self.text_encoder(uncond_inputs)[0]\n",
        "\n",
        "      prompt_embeds = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      timesteps = self.scheduler.timesteps[t_start:]\n",
        "      for t in self.scheduler.timesteps:\n",
        "        print(t)\n",
        "\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        noise_pred = unet(latent_model_input, t,prompt_embeds).sample\n",
        "\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # guidance scale\n",
        "\n",
        "\n",
        "\n",
        "        latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "\n",
        "\n",
        "    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[0]\n",
        "\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    image = (image * 255).round().astype(\"uint8\")\n",
        "    return [Image.fromarray(img) for img in image]\n",
        "\n",
        "  def generateFast(self,prompt,images_per_prompt,noise=None,diffusion_steps=50,tokenizer=None,text_encoder=None):\n",
        "    generater  = DiffusionPipeline.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            unet=self.unwrap_model(self.unet),\n",
        "            variant=PRECISION,\n",
        "        ).to(self.device)\n",
        "    return generater(prompt,num_inference_steps=diffusion_steps,num_images_per_prompt=images_per_prompt)\n",
        "\n",
        "  def save():\n",
        "    pass\n",
        "  def load():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1HOVddBZ8klW"
      },
      "outputs": [],
      "source": [
        "configs = {\n",
        "    \"lr\" : 1e-6 ,\n",
        "    \"adam_beta1\" : 0.9,\n",
        "    \"adam_beta2\" : 0.999,\n",
        "    \"betas\":(0.9,0.999),\n",
        "    \"adam_weight_decay\" : 1e-2,\n",
        "    \"adam_epsilon\" : 1e-08,\n",
        "    \"lr_scheduler\" : 'constant',\n",
        "    \"lr_warmup_steps\": 0,\n",
        "    \"lr_num_cycles\": 1,\n",
        "    \"max_train_steps\":1200,\n",
        "    \"lr_power\" : 1.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"mixed_precision\": \"fp16\",\n",
        "    \"report_to\": \"wandb\",\n",
        "    \"gradient_checkpointing\":True,\n",
        "    \"dir\": \"/content/dreambooth\"\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDM1WtAZC3ku",
        "outputId": "11a90638-77c5-47d2-f7fe-a67dd3a04fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Downloading and loading models ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:412: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "dreamBooth = DreamBoothPipeline(configs,False,False,accelerate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fnF8gGaczbo",
        "outputId": "434078c5-87d1-4bf4-9920-c0a19400b467"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/24:   0%|          | 0/51 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 0.8145638423807481\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 loss: 0.7119813608188256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 loss: 0.6340532285325667\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 loss: 0.37135886123367384\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 loss: 0.21604255207029044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 loss: 0.1909747758843735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/24:  16%|█▌        | 8/51 [00:15<01:20,  1.88s/it, loss=0.231, avg_loss=0.139]"
          ]
        }
      ],
      "source": [
        "dreamBooth.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jwwrHWPxc_RG",
        "outputId": "08a0d729-17d6-45ca-9948-071692018288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d368098b9263>:165: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
            "  init_latent = torch.zeros((images_per_prompt, unet.in_channels, 768 // 8, 768 // 8), device=self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(981)\n",
            "tensor(961)\n",
            "tensor(941)\n",
            "tensor(921)\n",
            "tensor(901)\n",
            "tensor(881)\n",
            "tensor(861)\n",
            "tensor(841)\n",
            "tensor(821)\n",
            "tensor(801)\n",
            "tensor(781)\n",
            "tensor(761)\n",
            "tensor(741)\n",
            "tensor(721)\n",
            "tensor(701)\n",
            "tensor(681)\n",
            "tensor(661)\n",
            "tensor(641)\n",
            "tensor(621)\n",
            "tensor(601)\n",
            "tensor(581)\n",
            "tensor(561)\n",
            "tensor(541)\n",
            "tensor(521)\n",
            "tensor(501)\n",
            "tensor(481)\n",
            "tensor(461)\n",
            "tensor(441)\n",
            "tensor(421)\n",
            "tensor(401)\n",
            "tensor(381)\n",
            "tensor(361)\n",
            "tensor(341)\n",
            "tensor(321)\n",
            "tensor(301)\n",
            "tensor(281)\n",
            "tensor(261)\n",
            "tensor(241)\n",
            "tensor(221)\n",
            "tensor(201)\n",
            "tensor(181)\n",
            "tensor(161)\n",
            "tensor(141)\n",
            "tensor(121)\n",
            "tensor(101)\n",
            "tensor(81)\n",
            "tensor(61)\n",
            "tensor(41)\n",
            "tensor(21)\n",
            "tensor(1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1005.06 MiB is free. Process 307179 has 13.76 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b890a19e07d4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdreamBooth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A  women with blue eyes and blonde hair  wearing cap\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-d368098b9263>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, images_per_prompt, noise, diffusion_steps, tokenizer, text_encoder)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/utils/accelerate_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_hf_hook\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pre_forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_quant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/autoencoders/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mup_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# post-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mupsampler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2750\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_conv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1005.06 MiB is free. Process 307179 has 13.76 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "images = dreamBooth.generate(\"A  women with blue eyes and blonde hair  wearing cap\",3,tokenizer=tokenizer,text_encoder=text_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqth-9lwmBZb"
      },
      "outputs": [],
      "source": [
        "\n",
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "P0-ihp1Vfl7e",
        "outputId": "9682f7fd-a895-4ea9-f21c-ed5bc8f13820"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Image' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-83a9a1190209>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Image' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "generate_image = Image.fromarray((images[1] * 255).astype(\"uint8\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ogU4RRLjfx5T"
      },
      "outputs": [],
      "source": [
        "generate_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tIm3MZr8DKvD"
      },
      "outputs": [],
      "source": [
        "from diffusers import  AutoencoderKL\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def inference(model, prompt, num_steps=50, output_image_path=\"generated_image.png\"):\n",
        "        # Tokenize the input prompt\n",
        "        inputs = model.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "\n",
        "        # Encode the text input to get text embeddings\n",
        "        text_embeddings = model.text_encoder(inputs)[0]\n",
        "        # Generate latent noise for the image\n",
        "        latents = torch.randn((1, self.unet.config.in_channels, 64, 64), device=self.device, dtype=self.dtype)\n",
        "\n",
        "        # Prepare for the denoising loop\n",
        "        self.scheduler.set_timesteps(num_steps)\n",
        "        latents = latents * self.scheduler.init_noise_sigma\n",
        "\n",
        "        # Denoising loop using the UNet model\n",
        "        for t in self.scheduler.timesteps:\n",
        "            latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # Decode the latents to pixel space using the VAE\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        # Convert the image to PIL format\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        image = Image.fromarray((image[0] * 255).astype(\"uint8\"))\n",
        "\n",
        "        # Save or display the generated image\n",
        "        image.save(output_image_path)\n",
        "        image.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9umAGJOzTBz8"
      },
      "outputs": [],
      "source": [
        "    sd_model.inference(prompt=\"A scenic landscape with mountains during sunset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvl1HLpypTWe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNyg0NfypRR4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icf12keWatBK"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvSyFYHveqZd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "prompts = [\"A young beautiful women with medium hair with a short smile\",\n",
        "          \"face of beatiful young indian lady with long hair dressed in saree  \",\n",
        "          \"A young  indian women dressed in shirt with bob hair \",\n",
        "          \"A beautiful women dressed formally wearing sunglases\",\n",
        "           \"A  women with blue eyes and blonde hair dressed in jeans wearing bag\",\n",
        "         \"A  women with brown eyes and straight hair dressed in traditional sitting in temple\",\n",
        "\n",
        "]\n",
        "prompts = prompts * 10\n",
        "pipe.to(\"cuda\")\n",
        "index = 0\n",
        "for prompt in prompts:\n",
        "  images = pipe(prompt=prompt, num_inference_steps=35, num_images_per_prompt = 8)[0]\n",
        "  for i in range(len(images)):\n",
        "          images[i].save(os.path.join(\"/content/classimgs\", f\"generated_{index}.png\"))\n",
        "          index +=1\n",
        "del(pipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arS6pCtllwSw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rku0-AX6iJj3"
      },
      "outputs": [],
      "source": [
        "  @staticmethod\n",
        "  def generate_class_images(class_images_dir, n_class:int, class_prompt:str, device=\"cpu\"):\n",
        "    if not os.path.exists(class_images_dir):\n",
        "      os.mkdir(class_images_dir)\n",
        "\n",
        "    if n_class > len(os.listdir(class_images_dir)):\n",
        "      print(\"Generating class images\")\n",
        "\n",
        "      vae = AutoencoderKL.from_pretrained(\n",
        "          \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "          torch_dtype=torch.float16\n",
        "      )\n",
        "      pipe = DiffusionPipeline.from_pretrained(\n",
        "          \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "          vae=vae,\n",
        "          torch_dtype=torch.float16,\n",
        "          variant=\"fp16\",\n",
        "          use_safetensors=True,\n",
        "      )\n",
        "      pipe.to(\"cuda\");\n",
        "\n",
        "      base_SD_pipeline = DiffusionPipeline.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
        "\n",
        "      images = base_SD_pipeline(prompt=class_prompt, num_inference_steps=25, num_images_per_prompt=n_class-len(os.listdir(class_images_dir)))\n",
        "      for i in range(len(images)):\n",
        "        images[i].save(os.path.join(class_images_dir, f\"generated_{i}.png\"))\n",
        "      del(base_SD_pipeline)\n",
        "\n",
        "    else:\n",
        "      print(\"Class images already generated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvCkcoRDA5-z"
      },
      "outputs": [],
      "source": [
        "# prompt: zip this folder /content/classimgs\n",
        "\n",
        "!zip -r /content/classimgs.zip /content/classimgs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqcyE_BTPhKE"
      },
      "source": [
        "**ERROR** :\n",
        "\n",
        "```\n",
        "ValueError: Attempting to unscale FP16 gradients.\n",
        "```\n",
        "\n",
        "use_safetensors=True during training this causes the error\n",
        "\n",
        " configurations may not work well with mixed precision when using safetensors\n",
        "\n",
        " ```\n",
        "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward```\n",
        "\n",
        "\n",
        "1) during inference of text encoder for prompt it creates a graph for backpass\n",
        "2) this graph is deleted during backpass of first batch\n",
        "3) so while second batch graph is not there\n",
        "4) so if we inference with no grad the op of text encoder will have no graph connection so backward pas will no backprogate there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8P3QdzhA9ff"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hn9vzyhVl7K"
      },
      "outputs": [],
      "source": [
        "# prompt: what does this do\n",
        "#         latents = self.scheduler.scale_model_input(noise, t)\n",
        "\n",
        "# It scales the input noise based on the current timestep (t) in the diffusion process.\n",
        "\n",
        "# Let's break down what's happening:\n",
        "\n",
        "# 1. `self.scheduler`: This refers to the scheduler object, which is responsible for managing the diffusion process. It contains methods for adding noise, removing noise, and scaling the input noise.\n",
        "\n",
        "# 2. `self.scheduler.scale_model_input(noise, t)`: This method takes the current noise and the current timestep as input and returns a scaled version of the noise.\n",
        "\n",
        "# The scaling is important because it helps the UNet model learn to predict the noise more effectively at different stages of the diffusion process. As the diffusion process progresses, the noise level decreases, and the model needs to be able to predict the noise more accurately at lower noise levels.\n",
        "\n",
        "# In essence, this line of code prepares the input noise for the UNet model by scaling it based on the current timestep. This ensures that the model receives appropriate input for each stage of the denoising process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iarFfZkaa6nU"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNtO9GvLyOlIcUdbNskz7mD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}